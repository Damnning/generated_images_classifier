{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f579d577-ed04-44a0-8b43-00ae848bdca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1e3cd8f7-7c0b-4b5a-854d-4ac1d0169d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])  # Remove the last layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        return features.view(features.size(0), -1)\n",
    "        \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TeacherStudentNetworks:\n",
    "    def __init__(self, feature_dim, output_dim):\n",
    "        self.teacher = SimpleNN(feature_dim, output_dim)\n",
    "        self.student = SimpleNN(feature_dim, output_dim)\n",
    "        \n",
    "class FeatureAugmenter:\n",
    "    def __init__(self, input_dim):\n",
    "        self.feature_augmenter = SimpleNN(input_dim, input_dim)\n",
    "        \n",
    "class BinaryClassifier:\n",
    "    def __init__(self, input_dim):\n",
    "        self.binary_classifier = SimpleNN(input_dim, 2)\n",
    "\n",
    "class RealFakeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "adcef093-210d-4579-afe9-6d6609d8d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_paths_and_labels(root_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for model in os.listdir(root_dir):\n",
    "        model_path = os.path.join(root_dir, model)\n",
    "        if os.path.isdir(model_path):\n",
    "            for label in ['0_real', '1_fake']:\n",
    "                label_path = os.path.join(model_path, label)\n",
    "                if os.path.isdir(label_path):\n",
    "                    for img_name in os.listdir(label_path):\n",
    "                        img_path = os.path.join(label_path, img_name)\n",
    "                        image_paths.append(img_path)\n",
    "                        labels.append(0 if '0_real' in label else 1)\n",
    "                else:\n",
    "                    for obj in os.listdir(model_path):\n",
    "                        obj_path = os.path.join(model_path, obj)\n",
    "                        label_path = os.path.join(obj_path, label)\n",
    "                        if os.path.isdir(label_path):\n",
    "                            for img_name in os.listdir(label_path):\n",
    "                                img_path = os.path.join(label_path, img_name)\n",
    "                                image_paths.append(img_path)\n",
    "                                labels.append(0 if '0_real' in label else 1)\n",
    "    \n",
    "    return image_paths, labels\n",
    "    \n",
    "def train_teacher(teacher, feature_extractor, dataloader, device, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(teacher.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    feature_extractor.to(device)\n",
    "    teacher.to(device)\n",
    "    teacher.train()\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(dataloader, desc=f'Teacher training {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = teacher(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "        save_checkpoint(teacher, filename='teacher_cpt', epoch=epoch)\n",
    "        \n",
    "    print('Finished Training Teacher')\n",
    "    \n",
    "\n",
    "def train_studen_augmenter(teacher, student, feature_extractor, augmenter, dataloader, device, num_epochs=10, margin=1.0):\n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    optimizer_augmenter = optim.Adam(augmenter.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    augmenter.to(device)\n",
    "    feature_extractor.to(device)\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    feature_extractor.eval()\n",
    "    augmenter.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss_real = 0.0\n",
    "        running_loss_fake = 0.0\n",
    "        running_loss_augmenter = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(dataloader, desc=f'Student and Augmenter training {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            # Real images\n",
    "            real_indices = (labels == 0)\n",
    "            real_features = features[real_indices]\n",
    "            if len(real_features) > 0:\n",
    "                teacher_real = teacher(real_features)\n",
    "                student_real = student(real_features)\n",
    "                loss_real = torch.mean((teacher_real - student_real) ** 2)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_real.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss_real += loss_real.item()\n",
    "            \n",
    "            # Fake images\n",
    "            fake_indices = (labels == 1)\n",
    "            fake_features = features[fake_indices]\n",
    "            if len(fake_features) > 0:\n",
    "                augmented_fake_features = augmenter(fake_features)\n",
    "                teacher_fake = teacher(augmented_fake_features)\n",
    "                student_fake = student(augmented_fake_features)\n",
    "                discrepancy = torch.mean((teacher_fake / teacher_fake.norm(2, dim=1, keepdim=True) - student_fake / student_fake.norm(2, dim=1, keepdim=True)) ** 2)\n",
    "                loss_fake = torch.clamp(margin - discrepancy, min=0.0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_fake.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_augmenter = torch.mean((teacher_fake - student_fake) ** 2)\n",
    "                \n",
    "                optimizer_augmenter.zero_grad()\n",
    "                loss_augmenter.backward()\n",
    "                optimizer_augmenter.step()\n",
    "                \n",
    "                \n",
    "                running_loss_fake += loss_fake.item()\n",
    "                running_loss_augmenter+= loss_augmenter.item()\n",
    "                \n",
    "        print(f'Epoch {epoch+1}, Real Loss: {running_loss_real/len(dataloader)}, Fake Loss: {running_loss_fake/len(dataloader)}')\n",
    "        save_checkpoint(student, filename='student_cpt', epoch=epoch)\n",
    "        save_checkpoint(augmenter, filename='augmenter_cpt', epoch=epoch)\n",
    "        \n",
    "    print('Finished Training Student and Augmenter')\n",
    "        \n",
    "def train_student(teacher, student, feature_extractor, augmenter, dataloader, device, num_epochs=10, margin=1.0):\n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    augmenter.to(device)\n",
    "    feature_extractor.to(device)\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    feature_extractor.eval()\n",
    "    augmenter.eval()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss_real = 0.0\n",
    "        running_loss_fake = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(dataloader, desc=f'Student training {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            # Real images\n",
    "            real_indices = (labels == 0)\n",
    "            real_features = features[real_indices]\n",
    "            if len(real_features) > 0:\n",
    "                teacher_real = teacher(real_features)\n",
    "                student_real = student(real_features)\n",
    "                loss_real = torch.mean((teacher_real - student_real) ** 2)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_real.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss_real += loss_real.item()\n",
    "            \n",
    "            # Fake images\n",
    "            fake_indices = (labels == 1)\n",
    "            fake_features = features[fake_indices]\n",
    "            if len(fake_features) > 0:\n",
    "                augmented_fake_features = augmenter(fake_features)\n",
    "                teacher_fake = teacher(augmented_fake_features)\n",
    "                student_fake = student(augmented_fake_features)\n",
    "                discrepancy = torch.mean((teacher_fake / teacher_fake.norm(2, dim=1, keepdim=True) - student_fake / student_fake.norm(2, dim=1, keepdim=True)) ** 2)\n",
    "                loss_fake = torch.clamp(margin - discrepancy, min=0.0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_fake.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss_fake += loss_fake.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Real Loss: {running_loss_real/len(dataloader)}, Fake Loss: {running_loss_fake/len(dataloader)}')\n",
    "        save_checkpoint(student, filename='student_cpt', epoch=epoch)\n",
    "        \n",
    "    print('Finished Training Student')\n",
    "    \n",
    "def train_binary_classifier(teacher, student, classifier, feature_extractor, dataloader, device, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    classifier.to(device)\n",
    "    feature_extractor.to(device)\n",
    "    classifier.train()\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    feature_extractor.eval()\n",
    "    augmenter.eval()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(dataloader, desc=f'Classifier training {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(features)\n",
    "                student_outputs = student(features)\n",
    "            discrepancies = (teacher_outputs - student_outputs) ** 2\n",
    "            outputs = classifier(discrepancies)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "        save_checkpoint(augmenter, filename='bin_classifier_cpt', epoch=epoch)\n",
    "        \n",
    "    print('Finished Training Binary Classifier')\n",
    "\n",
    "\n",
    "def train_augmenter(teacher, student, feature_extractor, augmenter, dataloader, device, num_epochs=10):\n",
    "    optimizer = optim.Adam(augmenter.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    augmenter.to(device)\n",
    "    feature_extractor.to(device)\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    feature_extractor.eval()\n",
    "    augmenter.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in tqdm(dataloader, desc=f'Augmenter training {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            # Fake images\n",
    "            fake_indices = (labels == 1)\n",
    "            fake_features = features[fake_indices]\n",
    "            if len(fake_features) > 0:\n",
    "                augmented_fake_features = augmenter(fake_features)\n",
    "                with torch.no_grad:\n",
    "                    teacher_fake = teacher(augmented_fake_features)\n",
    "                    student_fake = student(augmented_fake_features)\n",
    "                loss = torch.mean((teacher_fake - student_fake) ** 2)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss+= loss.item()\n",
    "                \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "        save_checkpoint(augmenter, filename='augmenter_cpt', epoch=epoch)\n",
    "        \n",
    "    print('Finished Training Augmenter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6687c210-5ad5-4eff-bc9d-88daec7bdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "838c28c5-de5d-45a9-af4a-4d181e460200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, filename='checkpoint', epoch = 0):\n",
    "    modelfilename = filename + f'_epoch{epoch}.pth.tar'\n",
    "    torch.save(model.state_dict(), f'models/{modelfilename}')\n",
    "    \n",
    "    print(f'Saved checkpoint as: {modelfilename}')\n",
    "def load_checkpoint(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f'Loaded model: {path}')\n",
    "    \n",
    "def add_log(msg):\n",
    "     with open('log.txt', 'a') as file:\n",
    "        time = strftime(\"%H:%M\", gmtime())\n",
    "        file.write(time + ': ' + msg + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c12db88d-b139-48b7-868e-5dfa1b414a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image paths and labels\n",
    "image_paths, labels = load_image_paths_and_labels('C:/Users/Danila/VSU/vsu_common_rep/vsu_common_rep/2year/2term/project/image_classification/content/CNN_synth/train_set/')\n",
    "\n",
    "# Create a dataset instance \n",
    "full_dataset = RealFakeDataset(image_paths, labels, transform=transform)\n",
    "\n",
    "# Define the split ratio\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1abffd23-bc1d-46a4-a864-6c679cd33d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_dim = 2048  # ResNet50 output feature dimension\n",
    "teacher_student = TeacherStudentNetworks(feature_dim, 2)\n",
    "augmenter = FeatureAugmenter(feature_dim)\n",
    "binary_classifier = BinaryClassifier(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51195da8-3663-4922-9f1c-0f709cac1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_student.teacher.to(device)\n",
    "teacher_student.student.to(device)\n",
    "binary_classifier.binary_classifier.to(device)\n",
    "feature_extractor.to(device)\n",
    "augmenter.feature_augmenter.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df6a39-a3a4-4e47-b0ec-06818d7b3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the teacher network\n",
    "train_teacher(teacher_student.teacher, feature_extractor, train_loader, device, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "702b02a6-d3ae-430d-8dc7-3998bb36e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Student and Augmenter training 1/10:   0%|                                                 | 0/2259 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [512, 2]], which is output 0 of AsStridedBackward0, is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the student and the augmenter\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_studen_augmenter(teacher_student\u001b[38;5;241m.\u001b[39mteacher, teacher_student\u001b[38;5;241m.\u001b[39mstudent, feature_extractor, augmenter\u001b[38;5;241m.\u001b[39mfeature_augmenter, train_loader, device, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the binary classifier\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_binary_classifier(teacher_student\u001b[38;5;241m.\u001b[39mteacher, teacher_student\u001b[38;5;241m.\u001b[39mstudent, binary_classifier\u001b[38;5;241m.\u001b[39mbinary_classifier, feature_extractor, train_loader, device, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[115], line 108\u001b[0m, in \u001b[0;36mtrain_studen_augmenter\u001b[1;34m(teacher, student, feature_extractor, augmenter, dataloader, device, num_epochs, margin)\u001b[0m\n\u001b[0;32m    105\u001b[0m loss_augmenter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((teacher_fake \u001b[38;5;241m-\u001b[39m student_fake) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    107\u001b[0m optimizer_augmenter\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 108\u001b[0m loss_augmenter\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    109\u001b[0m optimizer_augmenter\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m running_loss_fake \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fake\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [512, 2]], which is output 0 of AsStridedBackward0, is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# Train the student and the augmenter\n",
    "train_studen_augmenter(teacher_student.teacher, teacher_student.student, feature_extractor, augmenter.feature_augmenter, train_loader, device, 10)\n",
    "\n",
    "# Train the binary classifier\n",
    "train_binary_classifier(teacher_student.teacher, teacher_student.student, binary_classifier.binary_classifier, feature_extractor, train_loader, device, 10)\n",
    "\n",
    "testacc = test_binary_classifier(teacher_student.teacher, teacher_student.student, binary_classifier.binary_classifier, feature_extractor, test_loader, device)\n",
    "mytestacc = test_binary_classifier(teacher_student.teacher, teacher_student.student, binary_classifier.binary_classifier, feature_extractor, mytest_loader, device)\n",
    "add_log(f'Test dataset accuracy: {testacc}')\n",
    "add_log(f'My test dataset accuracy: {mytestacc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0e1cd-d548-446a-843f-61b9c623f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 1/10: 100%|████████████████████████████████████████████████████| 2259/2259 [14:55<00:00,  2.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7866641403352755\n",
      "Saved checkpoint as: teacher_cpt_epoch0.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 2/10: 100%|████████████████████████████████████████████████████| 2259/2259 [15:18<00:00,  2.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7466870907029519\n",
      "Saved checkpoint as: teacher_cpt_epoch1.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 3/10: 100%|████████████████████████████████████████████████████| 2259/2259 [15:05<00:00,  2.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7773583210533095\n",
      "Saved checkpoint as: teacher_cpt_epoch2.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 4/10: 100%|████████████████████████████████████████████████████| 2259/2259 [14:05<00:00,  2.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.764106681622357\n",
      "Saved checkpoint as: teacher_cpt_epoch3.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 5/10: 100%|████████████████████████████████████████████████████| 2259/2259 [13:55<00:00,  2.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.7977485331406157\n",
      "Saved checkpoint as: teacher_cpt_epoch4.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 6/10: 100%|████████████████████████████████████████████████████| 2259/2259 [13:57<00:00,  2.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.7842030296973709\n",
      "Saved checkpoint as: teacher_cpt_epoch5.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 7/10: 100%|████████████████████████████████████████████████████| 2259/2259 [13:52<00:00,  2.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.7754422654985273\n",
      "Saved checkpoint as: teacher_cpt_epoch6.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 8/10: 100%|████████████████████████████████████████████████████| 2259/2259 [13:53<00:00,  2.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.7679152669184711\n",
      "Saved checkpoint as: teacher_cpt_epoch7.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 9/10: 100%|████████████████████████████████████████████████████| 2259/2259 [13:54<00:00,  2.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7852224255621354\n",
      "Saved checkpoint as: teacher_cpt_epoch8.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Teacher training 10/10: 100%|███████████████████████████████████████████████████| 2259/2259 [13:55<00:00,  2.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.7877704605506342\n",
      "Saved checkpoint as: teacher_cpt_epoch9.pth.tar\n",
      "Finished Training Teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Student and Augmenter training 1/10:   0%|                                                 | 0/2259 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_teacher(teacher_student\u001b[38;5;241m.\u001b[39mteacher, feature_extractor, train_loader, device, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train the student and the augmenter\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_studen_augmenter(teacher_student\u001b[38;5;241m.\u001b[39mteacher, teacher_student\u001b[38;5;241m.\u001b[39mstudent, feature_extractor, augmenter\u001b[38;5;241m.\u001b[39mfeature_augmenter, train_loader, device, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the binary classifier\u001b[39;00m\n\u001b[0;32m     16\u001b[0m train_binary_classifier(teacher_student\u001b[38;5;241m.\u001b[39mteacher, teacher_student\u001b[38;5;241m.\u001b[39mstudent, binary_classifier\u001b[38;5;241m.\u001b[39mbinary_classifier, feature_extractor, train_loader, device, \u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[97], line 108\u001b[0m, in \u001b[0;36mtrain_studen_augmenter\u001b[1;34m(teacher, student, feature_extractor, augmenter, dataloader, device, num_epochs, margin)\u001b[0m\n\u001b[0;32m    105\u001b[0m loss_augmenter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((teacher_fake \u001b[38;5;241m-\u001b[39m student_fake) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    107\u001b[0m optimizer_augmenter\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 108\u001b[0m loss_augmenter\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    109\u001b[0m optimizer_augmenter\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m running_loss_fake \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fake\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697215a-9264-48a3-ba05-d5d43209eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(10):\n",
    "#     # Train the student network\n",
    "#     train_student(teacher_student.teacher, teacher_student.student, feature_extractor, augmenter, train_loader, device, 1, margin= 1.0)\n",
    "#     save_checkpoint(teacher_student.teacher, filename='student_cpt', epoch=i)\n",
    "    \n",
    "#     # Traun the augmenter\n",
    "#     train_augmenter(teacher_student.teacher, teacher_student.student, feature_extractor, augmenter, train_loader, device, 1)\n",
    "#     save_checkpoint(teacher_student.teacher, filename='augmenter_cpt', epoch=i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dcb2c2b-e973-486b-a823-0c4fb620f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_binary_classifier(teacher, student, classifier, feature_extractor, dataloader, device):\n",
    "    classifier.to(device)\n",
    "    teacher.to(device) \n",
    "    student.to(device) \n",
    "    feature_extractor.to(device) \n",
    "    classifier.eval() # Set the classifier to evaluation mode\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            # Get outputs from teacher and student networks\n",
    "            teacher_outputs = teacher(features)\n",
    "            student_outputs = student(features)\n",
    "            \n",
    "            # Calculate discrepancy\n",
    "            discrepancy = (teacher_outputs - student_outputs) ** 2\n",
    "            \n",
    "            # Get predictions from binary classifier\n",
    "            outputs = classifier(discrepancy)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db419e9f-7021-4674-95fa-dba7d66d856b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object does not support the context manager protocol",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_binary_classifier(teacher_student\u001b[38;5;241m.\u001b[39mteacher, teacher_student\u001b[38;5;241m.\u001b[39mstudent, binary_classifier, feature_extractor, test_loader, device)\n",
      "Cell \u001b[1;32mIn[23], line 17\u001b[0m, in \u001b[0;36mtest_binary_classifier\u001b[1;34m(teacher, student, classifier, feature_extractor, dataloader, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     16\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         features \u001b[38;5;241m=\u001b[39m feature_extractor(images)\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# Get outputs from teacher and student networks\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object does not support the context manager protocol"
     ]
    }
   ],
   "source": [
    "test_binary_classifier(teacher_student.teacher, teacher_student.student, binary_classifier, feature_extractor, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6ab4b9-4b68-419d-bda8-e7339e2e84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytest_image_paths, mytest_labels = load_image_paths_and_labels(\"C:/Users/Danila/VSU/vsu_common_rep/vsu_common_rep/2year/2term/project/image_classification/content/CNN_synth/test_set/\")\n",
    "mytest_dataset = RealFakeDataset(mytest_image_paths, mytest_labels, transform=transform)\n",
    "mytest_loader = DataLoader(mytest_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079566da-d32d-4b96-8113-746aa78b4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.07%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4906609195402299"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_binary_classifier(teacher_student.teacher, teacher_student.student, binary_classifier, feature_extractor, mytest_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2ad4b-c5e4-4f37-91f6-be5030639bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize networks\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_dim = 2048  # ResNet50 output feature dimension\n",
    "teacher_student = TeacherStudentNetworks(feature_dim, 512)\n",
    "augmenter = FeatureAugmenter(feature_dim)\n",
    "binary_classifier = BinaryClassifier(2048)\n",
    "\n",
    "\n",
    "load_checkpoint(teacher_student.student, 'models_before_augmenter_training/student_cpt_epoch6.pth.tar')\n",
    "load_checkpoint(teacher_student.teacher, 'models_before_augmenter_training/teacher_cpt_epoch6.pth.tar')\n",
    "\n",
    "load_checkpoint(binary_classifier, 'models_before_augmenter_training/bin_classifier_cpt_epoch6.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "237887d0-fa3b-4d46-8faf-75af5f56927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint as: teacher_test_cpt_epoch0.pth.tar\n",
      "Saved checkpoint as: student_test_cpt_epoch0.pth.tar\n",
      "Saved checkpoint as: augmenter_test_cpt_epoch0.pth.tar\n",
      "Saved checkpoint as: bin_classifier_test_cpt_epoch0.pth.tar\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(teacher_student.teacher, filename='teacher_test_cpt', epoch=0)\n",
    "save_checkpoint(teacher_student.student, filename='student_test_cpt', epoch=0)\n",
    "save_checkpoint(augmenter.feature_augmenter, filename='augmenter_test_cpt', epoch=0)\n",
    "save_checkpoint(binary_classifier.binary_classifier, filename='bin_classifier_test_cpt', epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "81295948-f34b-4b7b-b6ab-519a5086f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: models/teacher_test_cpt_epoch0.pth.tar\n",
      "Loaded model: models/student_test_cpt_epoch0.pth.tar\n",
      "Loaded model: models/augmenter_test_cpt_epoch0.pth.tar\n",
      "Loaded model: models/bin_classifier_test_cpt_epoch0.pth.tar\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(teacher_student.student, 'models/teacher_test_cpt_epoch0.pth.tar')\n",
    "load_checkpoint(teacher_student.teacher, 'models/student_test_cpt_epoch0.pth.tar')\n",
    "load_checkpoint(augmenter.feature_augmenter, 'models/augmenter_test_cpt_epoch0.pth.tar')\n",
    "load_checkpoint(binary_classifier.binary_classifier, 'models/bin_classifier_test_cpt_epoch0.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a33b241-f285-4028-8546-c88871444f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Student training 1/1:   1%|▋                                                      | 26/2259 [00:10<14:41,  2.53batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_augmenter(teacher_student\u001b[38;5;241m.\u001b[39mteacher, teacher_student\u001b[38;5;241m.\u001b[39mstudent, feature_extractor, augmenter, train_loader, device, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 160\u001b[0m, in \u001b[0;36mtrain_augmenter\u001b[1;34m(teacher, student, feature_extractor, augmenter, dataloader, device, num_epochs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    158\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudent training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    161\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    162\u001b[0m         features \u001b[38;5;241m=\u001b[39m feature_extractor(images)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m, in \u001b[0;36mRealFakeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     64\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m---> 65\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     66\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_augmenter(teacher_student.teacher, teacher_student.student, feature_extractor, augmenter, train_loader, device, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9fee0a4-0903-4c8f-a5fb-190fcf24dd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca8cff-f5d5-4c94-92c3-f22e77a13472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
